"""
å·¥ä½œæµç®¡ç†å™¨
æ•´åˆæ‰€æœ‰æ¨¡å—ï¼šå˜å¼‚è§£æ â†’ å¼•ç‰©è®¾è®¡ â†’ BLASTéªŒè¯ â†’ å¯è§†åŒ– â†’ æŠ¥å‘Šç”Ÿæˆ
"""
import os
import logging
import json
import yaml
import pandas as pd
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, asdict, field
from typing import Dict, List, Optional, Tuple, Union, Any, Callable
from concurrent.futures import ThreadPoolExecutor, as_completed

from ..core.variant_parser import VariantParser, parse_variants
from ..core.primer_designer import PrimerDesigner, design_primers_from_dataframe
from ..core.specificity_checker import SpecificityChecker, check_primers_specificity
from ..visualization.gel_electrophoresis import GelElectrophoresisVisualizer
from ..visualization.recombination_plots import RecombinationPlotter
from ..config.defaults import DEFAULT_CONFIG, get_config, validate_config
from ..utils.file_utils import FileUtils
from ..utils.logging_utils import setup_logging

logger = logging.getLogger(__name__)


@dataclass
class WorkflowStep:
    """å·¥ä½œæµæ­¥éª¤å®šä¹‰"""
    name: str
    description: str
    function: Callable
    enabled: bool = True
    depends_on: List[str] = field(default_factory=list)
    timeout: int = 3600  # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    retry_count: int = 3
    
    def execute(self, *args, **kwargs):
        """æ‰§è¡Œæ­¥éª¤"""
        if not self.enabled:
            logger.info(f"è·³è¿‡æ­¥éª¤: {self.name}")
            return None
        
        logger.info(f"å¼€å§‹æ­¥éª¤: {self.name}")
        logger.debug(f"æ­¥éª¤æè¿°: {self.description}")
        
        for attempt in range(self.retry_count):
            try:
                result = self.function(*args, **kwargs)
                logger.info(f"æ­¥éª¤å®Œæˆ: {self.name}")
                return result
            except Exception as e:
                logger.warning(f"æ­¥éª¤ {self.name} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {str(e)}")
                if attempt == self.retry_count - 1:
                    logger.error(f"æ­¥éª¤ {self.name} æœ€ç»ˆå¤±è´¥: {str(e)}")
                    raise
        return None


@dataclass
class WorkflowConfig:
    """å·¥ä½œæµé…ç½®"""
    # è¾“å…¥æ–‡ä»¶
    variants_file: str
    reference_fasta: str
    blast_db: str
    
    # è¾“å‡ºè®¾ç½®
    output_dir: str = "./qtl_results"
    sample_prefix: str = "QTL"
    create_subdirectories: bool = True
    
    # å¤„ç†å‚æ•°
    max_workers: int = 8
    batch_size: int = 50
    save_intermediate: bool = True
    
    # æ¨¡å—é…ç½®
    variant_parser_config: Dict = field(default_factory=dict)
    primer_designer_config: Dict = field(default_factory=dict)
    specificity_checker_config: Dict = field(default_factory=dict)
    visualization_config: Dict = field(default_factory=dict)
    
    # æ­¥éª¤æ§åˆ¶
    enabled_steps: List[str] = field(default_factory=lambda: [
        'parse_variants',
        'design_primers',
        'check_specificity',
        'create_visualizations',
        'generate_reports'
    ])
    
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, config_dict: Dict) -> 'WorkflowConfig':
        """ä»å­—å…¸åˆ›å»ºé…ç½®"""
        return cls(**config_dict)
    
    def save(self, file_path: Union[str, Path]):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        file_path = Path(file_path)
        data = self.to_dict()
        
        if file_path.suffix in ['.yaml', '.yml']:
            with open(file_path, 'w') as f:
                yaml.dump(data, f, default_flow_style=False)
        elif file_path.suffix == '.json':
            with open(file_path, 'w') as f:
                json.dump(data, f, indent=2)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é…ç½®æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
        
        logger.info(f"é…ç½®å·²ä¿å­˜: {file_path}")


@dataclass
class WorkflowResult:
    """å·¥ä½œæµç»“æœ"""
    # åŸºæœ¬ä¿¡æ¯
    output_dir: Path
    start_time: datetime
    end_time: datetime
    success: bool
    
    # ç»“æœæ•°æ®
    config: WorkflowConfig
    indels_df: Optional[pd.DataFrame] = None
    primers_df: Optional[pd.DataFrame] = None
    blast_results_df: Optional[pd.DataFrame] = None
    visualization_results: Dict = field(default_factory=dict)
    report_files: Dict = field(default_factory=dict)
    
    # ç»Ÿè®¡ä¿¡æ¯
    statistics: Dict = field(default_factory=dict)
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    
    # æ€§èƒ½ä¿¡æ¯
    step_times: Dict = field(default_factory=dict)
    
    @property
    def duration(self):
        """è¿è¡Œæ—¶é•¿"""
        return self.end_time - self.start_time
    
    @property
    def indel_count(self):
        """Indelæ•°é‡"""
        if self.indels_df is not None:
            return len(self.indels_df)
        return 0
    
    @property
    def primer_count(self):
        """å¼•ç‰©æ•°é‡"""
        if self.primers_df is not None:
            return len(self.primers_df)
        return 0
    
    @property
    def successful_primer_count(self):
        """æˆåŠŸè®¾è®¡çš„å¼•ç‰©æ•°é‡"""
        if self.primers_df is not None:
            return len(self.primers_df[self.primers_df['QUALITY'] != 'FAILED'])
        return 0
    
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸"""
        result_dict = asdict(self)
        
        # è½¬æ¢Pathå¯¹è±¡ä¸ºå­—ç¬¦ä¸²
        result_dict['output_dir'] = str(self.output_dir)
        
        # è½¬æ¢DataFrameä¸ºå­—å…¸ï¼ˆä»…æ‘˜è¦ï¼‰
        if self.indels_df is not None:
            result_dict['indels_summary'] = {
                'count': len(self.indels_df),
                'columns': list(self.indels_df.columns),
                'head': self.indels_df.head().to_dict('records')
            }
        
        if self.primers_df is not None:
            result_dict['primers_summary'] = {
                'count': len(self.primers_df),
                'successful': len(self.primers_df[self.primers_df['QUALITY'] != 'FAILED']),
                'columns': list(self.primers_df.columns),
                'top_primers': self.primers_df.head(10).to_dict('records')
            }
        
        return result_dict
    
    def save_summary(self, file_path: Union[str, Path]):
        """ä¿å­˜æ‘˜è¦åˆ°æ–‡ä»¶"""
        file_path = Path(file_path)
        summary = self.to_dict()
        
        if file_path.suffix in ['.yaml', '.yml']:
            with open(file_path, 'w') as f:
                yaml.dump(summary, f, default_flow_style=False)
        elif file_path.suffix == '.json':
            with open(file_path, 'w') as f:
                json.dump(summary, f, indent=2)
        else:
            with open(file_path, 'w') as f:
                f.write(self._generate_text_summary())
        
        logger.info(f"ç»“æœæ‘˜è¦å·²ä¿å­˜: {file_path}")
    
    def _generate_text_summary(self) -> str:
        """ç”Ÿæˆæ–‡æœ¬æ‘˜è¦"""
        lines = []
        lines.append("=" * 60)
        lines.append("QTLç²¾ç»†å®šä½å¼•ç‰©è®¾è®¡å·¥ä½œæµ - ç»“æœæ‘˜è¦")
        lines.append("=" * 60)
        
        lines.append(f"\nåŸºæœ¬ä¿¡æ¯:")
        lines.append(f"  è¾“å‡ºç›®å½•: {self.output_dir}")
        lines.append(f"  å¼€å§‹æ—¶é—´: {self.start_time}")
        lines.append(f"  ç»“æŸæ—¶é—´: {self.end_time}")
        lines.append(f"  è¿è¡Œæ—¶é•¿: {self.duration}")
        lines.append(f"  æ˜¯å¦æˆåŠŸ: {'æ˜¯' if self.success else 'å¦'}")
        
        lines.append(f"\nç»“æœç»Ÿè®¡:")
        lines.append(f"  å€™é€‰Indelæ•°é‡: {self.indel_count}")
        lines.append(f"  è®¾è®¡å¼•ç‰©å¯¹æ•°: {self.primer_count}")
        lines.append(f"  æˆåŠŸè®¾è®¡å¼•ç‰©: {self.successful_primer_count}")
        
        if self.statistics:
            lines.append(f"\nè¯¦ç»†ç»Ÿè®¡:")
            for key, value in self.statistics.items():
                lines.append(f"  {key}: {value}")
        
        if self.errors:
            lines.append(f"\né”™è¯¯ä¿¡æ¯ ({len(self.errors)} ä¸ª):")
            for error in self.errors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                lines.append(f"  - {error}")
            if len(self.errors) > 5:
                lines.append(f"  ... è¿˜æœ‰ {len(self.errors) - 5} ä¸ªé”™è¯¯")
        
        if self.warnings:
            lines.append(f"\nè­¦å‘Šä¿¡æ¯ ({len(self.warnings)} ä¸ª):")
            for warning in self.warnings[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªè­¦å‘Š
                lines.append(f"  - {warning}")
        
        lines.append(f"\nç”Ÿæˆæ–‡ä»¶:")
        for file_type, file_path in self.report_files.items():
            lines.append(f"  {file_type}: {file_path}")
        
        lines.append(f"\næ­¥éª¤è€—æ—¶:")
        for step, duration in self.step_times.items():
            lines.append(f"  {step}: {duration:.2f}ç§’")
        
        lines.append("\n" + "=" * 60)
        return "\n".join(lines)


class QTLFineMappingWorkflow:
    """QTLç²¾ç»†å®šä½å®Œæ•´å·¥ä½œæµç®¡ç†å™¨"""
    
    def __init__(self, config: Optional[Union[Dict, WorkflowConfig]] = None):
        """
        åˆå§‹åŒ–å·¥ä½œæµç®¡ç†å™¨
        
        Args:
            config: å·¥ä½œæµé…ç½®
        """
        # å¤„ç†é…ç½®
        if config is None:
            self.config = WorkflowConfig(
                variants_file="",
                reference_fasta="",
                blast_db=""
            )
        elif isinstance(config, dict):
            self.config = WorkflowConfig.from_dict(config)
        elif isinstance(config, WorkflowConfig):
            self.config = config
        else:
            raise TypeError(f"ä¸æ”¯æŒçš„é…ç½®ç±»å‹: {type(config)}")
        
        # å·¥ä½œæµæ­¥éª¤
        self.steps = self._initialize_steps()
        self.results = {}
        self.current_step = 0
        self.total_steps = len(self.steps)
        
        # æ€§èƒ½ç›‘æ§
        self.start_time = None
        self.step_start_time = None
        
        logger.info("QTLç²¾ç»†å®šä½å·¥ä½œæµç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_steps(self) -> Dict[str, WorkflowStep]:
        """åˆå§‹åŒ–å·¥ä½œæµæ­¥éª¤"""
        steps = {}
        
        # æ­¥éª¤1: è§£æå˜å¼‚
        steps['parse_variants'] = WorkflowStep(
            name="parse_variants",
            description="è§£æå˜å¼‚æ–‡ä»¶ï¼Œæå–å€™é€‰Indel",
            function=self._step_parse_variants,
            enabled='parse_variants' in self.config.enabled_steps
        )
        
        # æ­¥éª¤2: è®¾è®¡å¼•ç‰©
        steps['design_primers'] = WorkflowStep(
            name="design_primers",
            description="ä¸ºå€™é€‰Indelè®¾è®¡PCRå¼•ç‰©",
            function=self._step_design_primers,
            enabled='design_primers' in self.config.enabled_steps,
            depends_on=['parse_variants']
        )
        
        # æ­¥éª¤3: BLASTç‰¹å¼‚æ€§éªŒè¯
        steps['check_specificity'] = WorkflowStep(
            name="check_specificity",
            description="ä½¿ç”¨BLASTéªŒè¯å¼•ç‰©ç‰¹å¼‚æ€§",
            function=self._step_check_specificity,
            enabled='check_specificity' in self.config.enabled_steps,
            depends_on=['design_primers']
        )
        
        # æ­¥éª¤4: åˆ›å»ºå¯è§†åŒ–
        steps['create_visualizations'] = WorkflowStep(
            name="create_visualizations",
            description="ç”Ÿæˆç”µæ³³æ¨¡æ‹Ÿå›¾å’Œé‡ç»„å›¾",
            function=self._step_create_visualizations,
            enabled='create_visualizations' in self.config.enabled_steps,
            depends_on=['check_specificity']
        )
        
        # æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Š
        steps['generate_reports'] = WorkflowStep(
            name="generate_reports",
            description="ç”Ÿæˆç»¼åˆæŠ¥å‘Š",
            function=self._step_generate_reports,
            enabled='generate_reports' in self.config.enabled_steps,
            depends_on=['create_visualizations']
        )
        
        return steps
    
    def run(self, variants_file: Optional[str] = None,
            reference_fasta: Optional[str] = None,
            blast_db: Optional[str] = None,
            output_dir: Optional[str] = None) -> WorkflowResult:
        """
        è¿è¡Œå®Œæ•´å·¥ä½œæµ
        
        Args:
            variants_file: å˜å¼‚æ–‡ä»¶è·¯å¾„ï¼ˆè¦†ç›–é…ç½®ï¼‰
            reference_fasta: å‚è€ƒåŸºå› ç»„è·¯å¾„ï¼ˆè¦†ç›–é…ç½®ï¼‰
            blast_db: BLASTæ•°æ®åº“è·¯å¾„ï¼ˆè¦†ç›–é…ç½®ï¼‰
            output_dir: è¾“å‡ºç›®å½•ï¼ˆè¦†ç›–é…ç½®ï¼‰
            
        Returns:
            WorkflowResult: å·¥ä½œæµç»“æœ
        """
        # æ›´æ–°é…ç½®
        if variants_file:
            self.config.variants_file = variants_file
        if reference_fasta:
            self.config.reference_fasta = reference_fasta
        if blast_db:
            self.config.blast_db = blast_db
        if output_dir:
            self.config.output_dir = output_dir
        
        # éªŒè¯å¿…éœ€å‚æ•°
        if not all([self.config.variants_file, self.config.reference_fasta, self.config.blast_db]):
            raise ValueError("å¿…éœ€å‚æ•°ç¼ºå¤±: variants_file, reference_fasta, blast_db")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir_path = Path(self.config.output_dir)
        if self.config.create_subdirectories:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir_path = output_dir_path / f"{self.config.sample_prefix}_{timestamp}"
        
        output_dir_path.mkdir(parents=True, exist_ok=True)
        self.config.output_dir = str(output_dir_path)
        
        # ä¿å­˜é…ç½®
        config_file = output_dir_path / "workflow_config.yaml"
        self.config.save(config_file)
        
        # åˆå§‹åŒ–ç»“æœ
        self.start_time = datetime.now()
        result = WorkflowResult(
            output_dir=output_dir_path,
            start_time=self.start_time,
            end_time=self.start_time,
            success=False,
            config=self.config
        )
        
        # è¿è¡Œå·¥ä½œæµ
        try:
            logger.info("=" * 60)
            logger.info("å¼€å§‹QTLç²¾ç»†å®šä½å·¥ä½œæµ")
            logger.info("=" * 60)
            logger.info(f"å˜å¼‚æ–‡ä»¶: {self.config.variants_file}")
            logger.info(f"å‚è€ƒåŸºå› ç»„: {self.config.reference_fasta}")
            logger.info(f"BLASTæ•°æ®åº“: {self.config.blast_db}")
            logger.info(f"è¾“å‡ºç›®å½•: {self.config.output_dir}")
            logger.info("=" * 60)
            
            # æ‰§è¡Œæ­¥éª¤
            for step_name, step in self.steps.items():
                if not step.enabled:
                    logger.info(f"è·³è¿‡ç¦ç”¨çš„æ­¥éª¤: {step_name}")
                    continue
                
                # æ£€æŸ¥ä¾èµ–
                for dep in step.depends_on:
                    if dep not in self.results:
                        raise RuntimeError(f"æ­¥éª¤ {step_name} ç¼ºå°‘ä¾èµ–: {dep}")
                
                # æ‰§è¡Œæ­¥éª¤
                step_start = datetime.now()
                try:
                    step_result = step.execute()
                    step_end = datetime.now()
                    step_duration = (step_end - step_start).total_seconds()
                    
                    self.results[step_name] = step_result
                    result.step_times[step_name] = step_duration
                    
                    logger.info(f"æ­¥éª¤ {step_name} å®Œæˆï¼Œè€—æ—¶ {step_duration:.2f} ç§’")
                    
                except Exception as e:
                    logger.error(f"æ­¥éª¤ {step_name} æ‰§è¡Œå¤±è´¥: {e}")
                    result.errors.append(f"{step_name}: {str(e)}")
                    raise
            
            # æ”¶é›†ç»“æœ
            self._collect_results(result)
            result.success = True
            
            logger.info("=" * 60)
            logger.info("ğŸ‰ å·¥ä½œæµæˆåŠŸå®Œæˆ!")
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            result.errors.append(f"å·¥ä½œæµå¤±è´¥: {str(e)}")
            result.success = False
        
        finally:
            # è®¾ç½®ç»“æŸæ—¶é—´
            result.end_time = datetime.now()
            duration = (result.end_time - result.start_time).total_seconds()
            logger.info(f"æ€»è¿è¡Œæ—¶é—´: {duration:.2f} ç§’")
            
            # ä¿å­˜ç»“æœæ‘˜è¦
            summary_file = output_dir_path / "workflow_summary.txt"
            result.save_summary(summary_file)
        
        return result
    
    def _step_parse_variants(self):
        """æ­¥éª¤1: è§£æå˜å¼‚"""
        logger.info("æ­¥éª¤1: è§£æå˜å¼‚æ–‡ä»¶")
        
        parser = VariantParser(config=self.config.variant_parser_config)
        df = parser.parse(self.config.variants_file)
        
        # è¿‡æ»¤indel
        indel_df = parser.filter_indels(df)
        
        if indel_df.empty:
            logger.warning("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„indelå˜å¼‚")
        
        # ä¿å­˜ç»“æœ
        if self.config.save_intermediate:
            output_file = Path(self.config.output_dir) / "01_indels_filtered.csv"
            indel_df.to_csv(output_file, index=False)
            logger.info(f"Indelç»“æœå·²ä¿å­˜: {output_file}")
        
        return {
            'parser': parser,
            'df': df,
            'indel_df': indel_df,
            'output_file': str(output_file) if self.config.save_intermediate else None
        }
    
    def _step_design_primers(self):
        """æ­¥éª¤2: è®¾è®¡å¼•ç‰©"""
        logger.info("æ­¥éª¤2: è®¾è®¡å¼•ç‰©")
        
        # è·å–ä¸Šä¸€æ­¥ç»“æœ
        prev_result = self.results.get('parse_variants')
        if prev_result is None or prev_result['indel_df'] is None:
            raise ValueError("éœ€è¦å…ˆå®Œæˆå˜å¼‚è§£ææ­¥éª¤")
        
        indel_df = prev_result['indel_df']
        
        if indel_df.empty:
            logger.warning("æ²¡æœ‰Indelå¯ä¾›è®¾è®¡å¼•ç‰©")
            return {
                'designer': None,
                'primers_df': pd.DataFrame(),
                'output_file': None
            }
        
        # è®¾è®¡å¼•ç‰©
        designer = PrimerDesigner(
            reference_fasta=self.config.reference_fasta,
            config=self.config.primer_designer_config
        )
        
        primers_df = design_primers_from_dataframe(
            indel_df, self.config.reference_fasta,
            config=self.config.primer_designer_config
        )
        
        # ä¿å­˜ç»“æœ
        output_file = None
        if self.config.save_intermediate and not primers_df.empty:
            output_dir = Path(self.config.output_dir)
            output_file = output_dir / "02_primers_designed.csv"
            primers_df.to_csv(output_file, index=False)
            
            # ä¿å­˜FASTAæ–‡ä»¶
            fasta_file = output_dir / "02_primers_sequences.fasta"
            self._save_primers_fasta(primers_df, fasta_file)
            
            logger.info(f"å¼•ç‰©è®¾è®¡ç»“æœå·²ä¿å­˜: {output_file}")
        
        return {
            'designer': designer,
            'primers_df': primers_df,
            'output_file': output_file
        }
    
    def _step_check_specificity(self):
        """æ­¥éª¤3: BLASTç‰¹å¼‚æ€§éªŒè¯"""
        logger.info("æ­¥éª¤3: BLASTç‰¹å¼‚æ€§éªŒè¯")
        
        # è·å–ä¸Šä¸€æ­¥ç»“æœ
        prev_result = self.results.get('design_primers')
        if prev_result is None or prev_result['primers_df'] is None:
            raise ValueError("éœ€è¦å…ˆå®Œæˆå¼•ç‰©è®¾è®¡æ­¥éª¤")
        
        primers_df = prev_result['primers_df']
        
        if primers_df.empty:
            logger.warning("æ²¡æœ‰å¼•ç‰©éœ€è¦éªŒè¯ç‰¹å¼‚æ€§")
            return {
                'checker': None,
                'blast_results': pd.DataFrame(),
                'output_file': None
            }
        
        # å‡†å¤‡å¼•ç‰©æ•°æ®
        primer_pairs = []
        for _, row in primers_df.iterrows():
            if row['LEFT_PRIMER'] not in ['FAILED', '']:
                primer_pairs.append({
                    'Primer_Name': f"{row['ID']}_P{row['PAIR_INDEX']}_F",
                    'Sequence': row['LEFT_PRIMER'],
                    'PRIMER_PAIR_ID': f"{row['ID']}_P{row['PAIR_INDEX']}",
                    'CHR': row['CHR'],
                    'POS': row['POS'],
                    'PRODUCT_SIZE': row['PRODUCT_SIZE']
                })
            
            if row['RIGHT_PRIMER'] not in ['FAILED', '']:
                primer_pairs.append({
                    'Primer_Name': f"{row['ID']}_P{row['PAIR_INDEX']}_R",
                    'Sequence': row['RIGHT_PRIMER'],
                    'PRIMER_PAIR_ID': f"{row['ID']}_P{row['PAIR_INDEX']}",
                    'CHR': row['CHR'],
                    'POS': row['POS'],
                    'PRODUCT_SIZE': row['PRODUCT_SIZE']
                })
        
        primers_for_blast = pd.DataFrame(primer_pairs)
        
        # BLASTéªŒè¯
        checker = SpecificityChecker(
            blast_db=self.config.blast_db,
            config=self.config.specificity_checker_config
        )
        
        blast_results = checker.batch_blast_check(
            primer_pairs, 
            batch_size=self.config.batch_size
        )
        
        # ä¿å­˜ç»“æœ
        output_file = None
        if self.config.save_intermediate and not blast_results.empty:
            output_dir = Path(self.config.output_dir)
            output_file = output_dir / "03_primers_with_specificity.csv"
            blast_results.to_csv(output_file, index=False)
            logger.info(f"ç‰¹å¼‚æ€§éªŒè¯ç»“æœå·²ä¿å­˜: {output_file}")
        
        return {
            'checker': checker,
            'primers_for_blast': primers_for_blast,
            'blast_results': blast_results,
            'output_file': output_file
        }
    
    def _step_create_visualizations(self):
        """æ­¥éª¤4: åˆ›å»ºå¯è§†åŒ–"""
        logger.info("æ­¥éª¤4: åˆ›å»ºå¯è§†åŒ–")
        
        # è·å–ä¸Šä¸€æ­¥ç»“æœ
        prev_result = self.results.get('check_specificity')
        if prev_result is None:
            raise ValueError("éœ€è¦å…ˆå®Œæˆç‰¹å¼‚æ€§éªŒè¯æ­¥éª¤")
        
        blast_results = prev_result.get('blast_results', pd.DataFrame())
        
        if blast_results.empty:
            logger.warning("æ²¡æœ‰BLASTç»“æœå¯ç”¨äºå¯è§†åŒ–")
            return {'visualization_dir': None, 'files': {}}
        
        # åˆ›å»ºå¯è§†åŒ–ç›®å½•
        output_dir = Path(self.config.output_dir) / "04_visualizations"
        output_dir.mkdir(exist_ok=True)
        
        # å‡èƒ¶ç”µæ³³å¯è§†åŒ–
        gel_visualizer = GelElectrophoresisVisualizer(
            config=self.config.visualization_config
        )
        
        # é€‰æ‹©å‰Nä¸ªæœ€ä½³å¼•ç‰©è¿›è¡Œå¯è§†åŒ–
        top_n = 3
        if 'Specificity_Score' in blast_results.columns:
            top_primers = blast_results.nlargest(top_n, 'Specificity_Score')
        else:
            top_primers = blast_results.head(top_n)
        
        visualization_files = {}
        
        for idx, row in top_primers.iterrows():
            primer_id = row.get('Primer_Name', f'primer_{idx}').replace('_F', '').replace('_R', '')
            
            # ç”Ÿæˆä¸‰ç§åœºæ™¯çš„ç”µæ³³å›¾
            for scenario, (geno_a, geno_b) in gel_visualizer.genotype_examples.items():
                output_file = output_dir / f"{primer_id}_{scenario}.png"
                
                gel_visualizer.visualize_recombination_test(
                    marker_a_genotype=geno_a,
                    marker_b_genotype=geno_b,
                    marker_a_product_size=350,  # é»˜è®¤å¤§å°
                    marker_b_product_size=550,
                    sample_name=primer_id,
                    output_path=str(output_file)
                )
                
                visualization_files[f"{primer_id}_{scenario}"] = str(output_file)
        
        # ç”Ÿæˆæ•™å­¦å›¾
        tutorial_file = output_dir / "gel_electrophoresis_tutorial.png"
        gel_visualizer.create_tutorial_figure(str(tutorial_file))
        visualization_files['tutorial'] = str(tutorial_file)
        
        # é‡ç»„å›¾ï¼ˆå¦‚æœæœ‰ä½ç½®ä¿¡æ¯ï¼‰
        plotter = RecombinationPlotter(config=self.config.visualization_config)
        
        # ç¤ºä¾‹ï¼šåˆ›å»ºé‡ç»„åŒºé—´å›¾
        if 'CHR' in blast_results.columns and 'POS' in blast_results.columns:
            # è·å–ç¬¬ä¸€ä¸ªå¼•ç‰©çš„æŸ“è‰²ä½“å’Œä½ç½®ä¿¡æ¯
            first_primer = blast_results.iloc[0]
            chr_name = first_primer['CHR']
            
            # æ”¶é›†æ‰€æœ‰å¼•ç‰©çš„ä½ç½®
            positions = []
            marker_names = []
            
            for _, row in blast_results.iterrows():
                if 'POS' in row and row['POS']:
                    positions.append(int(row['POS']))
                    marker_names.append(row['Primer_Name'])
            
            if len(positions) >= 2:
                plot_file = output_dir / "recombination_plot.png"
                plotter.plot_recombination_interval(
                    chromosome=chr_name,
                    positions=positions[:10],  # é™åˆ¶æ•°é‡
                    marker_names=marker_names[:10],
                    output_file=str(plot_file)
                )
                visualization_files['recombination_plot'] = str(plot_file)
        
        logger.info(f"åˆ›å»ºäº† {len(visualization_files)} ä¸ªå¯è§†åŒ–æ–‡ä»¶")
        
        return {
            'visualization_dir': str(output_dir),
            'files': visualization_files
        }
    
    def _step_generate_reports(self):
        """æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Š"""
        logger.info("æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Š")
        
        # æ”¶é›†æ‰€æœ‰æ­¥éª¤çš„ç»“æœ
        parse_result = self.results.get('parse_variants', {})
        design_result = self.results.get('design_primers', {})
        blast_result = self.results.get('check_specificity', {})
        viz_result = self.results.get('create_visualizations', {})
        
        # ç”ŸæˆæŠ¥å‘Š
        from .reporter import ReportGenerator
        
        reporter = ReportGenerator()
        report_dir = Path(self.config.output_dir) / "05_reports"
        report_dir.mkdir(exist_ok=True)
        
        reports = reporter.generate_all_reports(
            indels_df=parse_result.get('indel_df'),
            primers_df=design_result.get('primers_df'),
            blast_results_df=blast_result.get('blast_results'),
            visualization_files=viz_result.get('files', {}),
            output_dir=str(report_dir),
            config=self.config
        )
        
        logger.info(f"ç”Ÿæˆäº† {len(reports)} ä¸ªæŠ¥å‘Šæ–‡ä»¶")
        
        return {
            'report_dir': str(report_dir),
            'reports': reports
        }
    
    def _save_primers_fasta(self, primers_df: pd.DataFrame, output_path: Path):
        """ä¿å­˜å¼•ç‰©åºåˆ—ä¸ºFASTAæ ¼å¼"""
        with open(output_path, 'w') as f:
            for _, row in primers_df.iterrows():
                if row['LEFT_PRIMER'] not in ['FAILED', '']:
                    primer_id = f"{row['ID']}_P{row['PAIR_INDEX']}_F"
                    f.write(f">{primer_id}\n{row['LEFT_PRIMER']}\n")
                
                if row['RIGHT_PRIMER'] not in ['FAILED', '']:
                    primer_id = f"{row['ID']}_P{row['PAIR_INDEX']}_R"
                    f.write(f">{primer_id}\n{row['RIGHT_PRIMER']}\n")
    
    def _collect_results(self, result: WorkflowResult):
        """æ”¶é›†æ‰€æœ‰æ­¥éª¤çš„ç»“æœ"""
        # æ”¶é›†æ•°æ®
        parse_result = self.results.get('parse_variants', {})
        design_result = self.results.get('design_primers', {})
        blast_result = self.results.get('check_specificity', {})
        viz_result = self.results.get('create_visualizations', {})
        report_result = self.results.get('generate_reports', {})
        
        result.indels_df = parse_result.get('indel_df')
        result.primers_df = design_result.get('primers_df')
        result.blast_results_df = blast_result.get('blast_results')
        result.visualization_results = viz_result.get('files', {})
        result.report_files = report_result.get('reports', {})
        
        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        result.statistics = {
            'indel_count': result.indel_count,
            'primer_count': result.primer_count,
            'successful_primer_count': result.successful_primer_count,
            'visualization_count': len(result.visualization_results),
            'report_count': len(result.report_files)
        }
        
        if result.blast_results_df is not None and not result.blast_results_df.empty:
            if 'Specificity_Score' in result.blast_results_df.columns:
                avg_score = result.blast_results_df['Specificity_Score'].mean()
                result.statistics['average_specificity_score'] = f"{avg_score:.2f}"
            
            if 'Specificity_Grade' in result.blast_results_df.columns:
                grade_counts = result.blast_results_df['Specificity_Grade'].value_counts().to_dict()
                result.statistics['specificity_grades'] = grade_counts


# ä¾¿æ·å‡½æ•°
def run_qtl_fine_mapping_pipeline(
    variants_file: str,
    reference_fasta: str,
    blast_db: str,
    output_dir: str = "./qtl_results",
    config: Optional[Dict] = None
) -> WorkflowResult:
    """
    ä¸€é”®è¿è¡ŒQTLç²¾ç»†å®šä½å®Œæ•´æµç¨‹
    
    Args:
        variants_file: å˜å¼‚æ–‡ä»¶è·¯å¾„
        reference_fasta: å‚è€ƒåŸºå› ç»„è·¯å¾„
        blast_db: BLASTæ•°æ®åº“è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        config: é…ç½®å­—å…¸
        
    Returns:
        WorkflowResult: å·¥ä½œæµç»“æœ
    """
    # åˆ›å»ºé…ç½®
    workflow_config = WorkflowConfig(
        variants_file=variants_file,
        reference_fasta=reference_fasta,
        blast_db=blast_db,
        output_dir=output_dir
    )
    
    # åˆå¹¶ç”¨æˆ·é…ç½®
    if config:
        for key, value in config.items():
            if hasattr(workflow_config, key):
                setattr(workflow_config, key, value)
    
    # è¿è¡Œå·¥ä½œæµ
    workflow = QTLFineMappingWorkflow(workflow_config)
    result = workflow.run()
    
    return result


def run_workflow_from_config(config_file: str) -> WorkflowResult:
    """
    ä»é…ç½®æ–‡ä»¶è¿è¡Œå·¥ä½œæµ
    
    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        WorkflowResult: å·¥ä½œæµç»“æœ
    """
    config_file = Path(config_file)
    
    if not config_file.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
    
    # åŠ è½½é…ç½®
    if config_file.suffix in ['.yaml', '.yml']:
        with open(config_file, 'r') as f:
            config_dict = yaml.safe_load(f)
    elif config_file.suffix == '.json':
        with open(config_file, 'r') as f:
            config_dict = json.load(f)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„é…ç½®æ–‡ä»¶æ ¼å¼: {config_file.suffix}")
    
    # è¿è¡Œå·¥ä½œæµ
    workflow = QTLFineMappingWorkflow(config_dict)
    result = workflow.run()
    
    return result
